{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Bootcamp Microsoft Certification Challenge #1 - AI 102](https://assets.dio.me/79IKKjY5EHRPqlscNsYum7Iv9uQNa_siSO7Ab8Zv3II/f:webp/h:120/q:80/L3RyYWNrcy8wMzI1ZTE2Ni1mYTNjLTRmMWItYWNlMS04ZTdmM2M0ZDU4NDEucG5n)"
      ],
      "metadata": {
        "id": "nRiiEA255TY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio de projeto: Tradutor de Artigos Técnicos com AzureAI"
      ],
      "metadata": {
        "id": "Rvu-S-Uc5MOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenvolvendo uma solução de tradução de documentos e artigos utilizando o AzureAI."
      ],
      "metadata": {
        "id": "S-fj52lu6DZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalação e importação das libs necessárias"
      ],
      "metadata": {
        "id": "aTFm6lbPcDcC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClJ0ComSbwW6",
        "outputId": "65bcfd76-5c23-4c95-c381-7683e9e2ecc9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.0.0)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.5-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, tiktoken, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "Successfully installed langchain-core-0.3.15 langchain-openai-0.2.5 python-docx-1.1.2 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests python-docx  openai langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from docx import Document\n",
        "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "CwIbRPoOb9Oh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Azure Translate"
      ],
      "metadata": {
        "id": "k4z53jggcGy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_translation_request(text, target_language='pt-br', original_language='en'):\n",
        "\n",
        "    # Definir o caminho para o endpoint de tradução.\n",
        "    path = '/translate'\n",
        "    url = endpoint + path\n",
        "\n",
        "    # Configurar os cabeçalhos necessários para autenticação e tipo de conteúdo\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': subscription_key,   # Chave da API\n",
        "        'Ocp-Apim-Subscription-Region': location,        # Região\n",
        "        'Content-type': 'application/json',              # Tipo de conteúdo (JSON)\n",
        "        'X-ClientTraceId': str(os.urandom(16))           # Identificador único\n",
        "    }\n",
        "\n",
        "    # Corpo da solicitação, contendo o texto que será traduzido\n",
        "    body = [{'text': text}]\n",
        "\n",
        "    # Parâmetros da URL, definindo a versão da API e os idiomas de origem e destino\n",
        "    params = {\n",
        "        'api-version': '3.0',\n",
        "        'from': original_language,\n",
        "        'to': target_language\n",
        "    }\n",
        "\n",
        "    # Enviar a solicitação para o endpoint de tradução\n",
        "    request = requests.post(url, params=params, headers=headers, json=body)\n",
        "\n",
        "    # Retornar a resposta em formato JSON com o resultado da tradução\n",
        "    return request.json()\n",
        "\n",
        "\n",
        "def translate_text(text, target_language='pt-br', original_language='en'):\n",
        "    # Chama a função make_translation_request para fazer a tradução do texto\n",
        "    response = make_translation_request(text, target_language, original_language)\n",
        "\n",
        "    # Extrai e retorna o texto traduzido da resposta JSON da API\n",
        "    return response[0]['translations'][0]['text']\n",
        "\n",
        "\n",
        "def translate_document(path, target_language='pt-br', original_language='en'):\n",
        "    # Carrega o documento\n",
        "    doc = Document(path)\n",
        "    # Junta todos os parágrafos em um único texto\n",
        "    full_text = \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
        "\n",
        "    # Realiza a tradução fazendo uso da função make_translation_request\n",
        "    response = make_translation_request(full_text, target_language, original_language)\n",
        "    translated_text = response[0]['translations'][0]['text']\n",
        "\n",
        "    # Cria um novo documento e adiciona o texto traduzido\n",
        "    translated_doc = Document()\n",
        "    for line in translated_text.split(\"\\n\"):\n",
        "        translated_doc.add_paragraph(line)\n",
        "\n",
        "    # Salva o documento traduzido\n",
        "    path_translated = path.replace(\".docx\", f\"_{target_language}.docx\")\n",
        "    translated_doc.save(path_translated)\n",
        "\n",
        "    return path_translated"
      ],
      "metadata": {
        "id": "g7VB5owdxzt-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = 'https://api.cognitive.microsofttranslator.com'\n",
        "subscription_key = userdata.get('AZURE_TRANSLATE_SUBSCRIPTION_KEY')\n",
        "location = 'eastus'\n",
        "original_language = 'en-us'\n",
        "target_language = 'pt-br'"
      ],
      "metadata": {
        "id": "B7x_OQlg6QN-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o Azure Translate com uma estrofe da música Walk\n",
        "\n",
        "estrofe = \"\"\"A million miles away\n",
        "Your signal in the distance\n",
        "To whom it may concern\n",
        "I think I lost my way\n",
        "Getting good at starting over\n",
        "Every time that I return\"\"\"\n",
        "\n",
        "estrofe_traduzida = translate_text(estrofe)\n",
        "print(estrofe_traduzida)"
      ],
      "metadata": {
        "id": "Nu8QdEV1pYtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3097b5-b7a5-44ca-c01d-ffaa2b279cc6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Um milhão de milhas de distância\n",
            "Seu sinal à distância\n",
            "A quem possa interessar\n",
            "Eu acho que perdi meu caminho\n",
            "Ficando bom em começar de novo\n",
            "Toda vez que eu voltar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o Azure Translate com o documento com a música Walk\n",
        "\n",
        "document_path = '/content/walk.docx'\n",
        "translated_document_path = translate_document(document_path)\n",
        "print(translated_document_path)\n",
        "\n",
        "musica = Document(translated_document_path)\n",
        "musica = \"\\n\".join(paragraph.text for paragraph in musica.paragraphs)\n",
        "print()\n",
        "print(musica)"
      ],
      "metadata": {
        "id": "JCzHzdLh8cVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a951e9-111a-4c9f-889f-1e590aaeca37"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/walk_pt-br.docx\n",
            "\n",
            "Um milhão de milhas de distância\n",
            "Seu sinal à distância\n",
            "A quem possa interessar\n",
            "Eu acho que perdi meu caminho\n",
            "Ficando bom em começar de novo\n",
            "Toda vez que eu voltar\n",
            "Aprendendo a andar novamente\n",
            "Eu acredito que já esperei o suficiente\n",
            "Por onde eu começo?\n",
            "Aprendendo a falar novamente\n",
            "Você não pode ver que eu esperei o suficiente?\n",
            "Por onde eu começo?\n",
            "Você se lembra dos dias?\n",
            "Nós construímos essas montanhas de papel\n",
            "Então sentou-se e assistiu-os queimar\n",
            "Eu acho que encontrei meu lugar\n",
            "Você não pode sentir isso ficando mais forte\n",
            "Pequenos conquistadores\n",
            "Aprendendo a andar novamente\n",
            "Eu acredito que já esperei o suficiente\n",
            "Por onde eu começo?\n",
            "Aprendendo a falar novamente\n",
            "Eu acredito que já esperei o suficiente\n",
            "Por onde eu começo?\n",
            "Agora\n",
            "Pela primeira vez\n",
            "Você não presta atenção\n",
            "Liberte-me, novamente\n",
            "Para se manter vivo, um momento de cada vez\n",
            "Isso ainda está dentro, um sussurro para um motim\n",
            "O sacrifício, o saber sobreviver\n",
            "Aquele primeiro declínio, outro estado de espírito\n",
            "Estou de joelhos, estou rezando por um sinal\n",
            "Para sempre, sempre, eu nunca quero morrer\n",
            "Eu nunca quero morrer\n",
            "Eu nunca quero morrer\n",
            "Eu estou de joelhos, eu nunca quero morrer\n",
            "Eu estou dançando no meu túmulo\n",
            "Eu estou correndo pelo fogo\n",
            "Para sempre, sempre que\n",
            "Eu nunca quero morrer\n",
            "Eu nunca quero sair\n",
            "Eu nunca vou dizer adeus\n",
            "Para sempre, sempre que\n",
            "Para sempre, sempre que\n",
            "Aprendendo a andar novamente\n",
            "Eu acredito que já esperei o suficiente\n",
            "Por onde eu começo?\n",
            "Aprendendo a falar novamente\n",
            "Você não pode ver que eu esperei o suficiente?\n",
            "Por onde eu começo?\n",
            "Aprendendo a andar novamente\n",
            "Eu acredito que já esperei o suficiente\n",
            "Aprendendo a falar novamente\n",
            "Você não pode ver que eu esperei o suficiente?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Azure OpenAI"
      ],
      "metadata": {
        "id": "iHals9bKpY8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_url(url):\n",
        "    \"\"\"\n",
        "    Extrai e limpa o texto de uma página web fornecida por meio de uma URL.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Requisição GET na URL passada\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Verifica se a requisição foi bem-sucedida (código de status 200)\n",
        "    if response.status_code == 200:\n",
        "        # Pega o conteúdo da página\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove elementos 'script' e 'style'\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.decompose()\n",
        "\n",
        "        # Extrai o texto da página, usando espaços como separador entre blocos\n",
        "        text = soup.get_text(separator=' ')\n",
        "\n",
        "        # Limpa o texto para remover espaços e linhas extras\n",
        "        lines = (line.strip() for line in text.splitlines())  # Remove espaços ao redor de cada linha\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))  # Remove espaços internos duplos\n",
        "\n",
        "        # Reúne o texto limpo em blocos, separados por quebras de linha\n",
        "        texto_limpo = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    else:\n",
        "        # Se der erro, printa o código de status HTTP\n",
        "        print(f\"Failed to fetch URL. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    # Retorna o texto limpo extraído da página\n",
        "    return texto_limpo\n",
        "\n",
        "\n",
        "def translate_article(text, lang, client):\n",
        "    \"\"\"\n",
        "    Traduz um artigo para o idioma especificado utilizando um cliente de tradução.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Define mensagens para o cliente\n",
        "    messages = [\n",
        "        (\"system\", \"Você atua como tradutor de textos\"),\n",
        "        (\"user\", f\"Traduza o {text} para o idioma {lang} e dê a resposta no formato markdown\")\n",
        "    ]\n",
        "\n",
        "    # Envia as mensagens para o cliente e obtém a resposta com o texto traduzido\n",
        "    response = client.invoke(messages)\n",
        "\n",
        "    # Retorna o conteúdo traduzido da resposta\n",
        "    return response.content\n"
      ],
      "metadata": {
        "id": "7rroAyBD1CGC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "azure_endpoint = 'https://gesiel-oai-eastus-001.openai.azure.com/'\n",
        "azure_openai_api_key = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "api_version = '2024-02-15-preview'\n",
        "deployment_name = 'gpt-4o-mini'\n",
        "\n",
        "\n",
        "client = AzureChatOpenAI(\n",
        "  azure_endpoint=azure_endpoint,\n",
        "  api_key=azure_openai_api_key,\n",
        "  api_version=api_version,\n",
        "  deployment_name=deployment_name,\n",
        "  max_retries=0\n",
        ")"
      ],
      "metadata": {
        "id": "jGavycLh4uyQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando com uma estrofe da música Walk\n",
        "\n",
        "text = \"\"\"A million miles away\n",
        "Your signal in the distance\n",
        "To whom it may concern\n",
        "I think I lost my way\n",
        "Getting good at starting over\n",
        "Every time that I return\"\"\"\n",
        "\n",
        "translated_text = translate_article(text, \"pt-br\", client)\n",
        "\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gytSW-SS2AWu",
        "outputId": "6ebe62af-27c6-4001-c5b6-24bfa1d2c6d7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claro! Aqui está a tradução em português:\n",
            "\n",
            "```markdown\n",
            "Um milhão de milhas de distância  \n",
            "Seu sinal na distância  \n",
            "A quem possa interessar  \n",
            "Acho que perdi meu caminho  \n",
            "Estou me saindo bem em recomeçar  \n",
            "Toda vez que retorno  \n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando com um artigo, limitando os caracteres para não etourar o limite de tokens\n",
        "url = 'https://dev.to/modernsystemdesign/llm-for-dummies-3h90'\n",
        "text = extract_text_from_url(url)[:800]\n",
        "\n",
        "translated_article = translate_article(text, \"pt-br\", client)\n",
        "print(translated_article)"
      ],
      "metadata": {
        "id": "G37yNSEw0GeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3802b1ba-8749-4ec8-a0a8-0be820ae45e1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# LLM para leigos\n",
            "\n",
            "Nos últimos anos, houve um avanço significativo em processamento de linguagem natural, levando ao desenvolvimento de modelos de linguagem de grande porte (LLMs). Esses modelos se tornaram cada vez mais populares devido à sua capacidade de processar e gerar linguagem semelhante à humana. Este artigo tem como objetivo explicar os LLMs para um novato, abordando tópicos como:\n",
            "\n",
            "## O que são LLMs?\n",
            "\n",
            "Modelos de linguagem de grande porte são algoritmos de aprendizado de máquina que foram treinados em grandes quantidades de texto. Eles são capazes de entender e gerar texto, respondendo a perguntas, completando frases e até mesmo escrevendo artigos.\n",
            "\n",
            "## Como funcionam?\n",
            "\n",
            "Os LLMs utilizam técnicas de aprendizado profundo, especificamente redes neurais, para analisar padrões na linguagem. Durante o treinamento, eles aprendem a prever a próxima palavra em uma sequência, o que lhes permite gerar texto coerente e relevante.\n",
            "\n",
            "## Aplicações\n",
            "\n",
            "Os LLMs têm uma ampla gama de aplicações, incluindo:\n",
            "\n",
            "- Assistentes virtuais (como o ChatGPT)\n",
            "- Tradução automática\n",
            "- Geração de conteúdo\n",
            "- Resumo de textos\n",
            "- Análise de sentimentos\n",
            "\n",
            "## Desafios\n",
            "\n",
            "Embora os LLMs sejam poderosos, eles também enfrentam desafios, como:\n",
            "\n",
            "- Geração de informações incorretas ou tendenciosas\n",
            "- Necessidade de grandes quantidades de dados para treinamento\n",
            "- Dificuldades em entender contexto e nuances\n",
            "\n",
            "## Futuro dos LLMs\n",
            "\n",
            "O futuro dos modelos de linguagem é promissor, com pesquisas contínuas focadas em melhorar a precisão e a ética no uso dessas tecnologias. À medida que a tecnologia avança, podemos esperar ver LLMs mais sofisticados e úteis em várias áreas.\n",
            "\n",
            "---\n",
            "\n",
            "Se você tem interesse em aprender mais sobre LLMs, continue explorando e experimentando com essas tecnologias fascinantes!\n"
          ]
        }
      ]
    }
  ]
}